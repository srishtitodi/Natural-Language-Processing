{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(D):\n",
    "    \"\"\"\n",
    "    Given a list of documents, where each document is represented as\n",
    "    a list of tokens, the vocabulary is returned. The vocabulary\n",
    "    is a set of tokens which appear more than once in the entire\n",
    "    document collection plus the \"<unk>\" token.\n",
    "    \"\"\"\n",
    "#   firstly, i have to convert the list of docs into a single list and then find the \n",
    "# vocab as a dictionary with the frequency in values. \n",
    "    \n",
    "    fin_doc=[]\n",
    "\n",
    "    fin_doc = list(itertools.chain(*D))\n",
    "    fin_doc_set = list(set(fin_doc))\n",
    "        \n",
    "    vocab_dict = {}\n",
    "    \n",
    "    for word in fin_doc:\n",
    "        if word in vocab_dict.keys():\n",
    "            vocab_dict[word]+=1\n",
    "        else:\n",
    "            vocab_dict[word] = 1\n",
    "\n",
    "    vocab_dict_final = {}\n",
    "    vocab_dict_final['<unk>'] = 0\n",
    "    \n",
    "    for key in vocab_dict.keys():\n",
    "        if vocab_dict[key] > 1:\n",
    "            vocab_dict_final[key] = vocab_dict[word]\n",
    "        elif vocab_dict[key] == 1:                     # mapping the values with freq =1 as <unk>\n",
    "            vocab_dict_final['<unk>']+=1\n",
    "            \n",
    "\n",
    "    \n",
    "    return vocab_dict_final\n",
    "    \n",
    "        \n",
    "    #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBoWFeaturizer(object):\n",
    "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
    "        \"\"\"\n",
    "        Given a document represented as a list of tokens and the vocabulary\n",
    "        as a set of tokens, computed the binary bag-of-words feature representation.\n",
    "        This function returns a dictionary which maps from the name of the\n",
    "        feature to the value of that feature.\n",
    "        \"\"\"\n",
    "\n",
    "    # if word is in doc and in vocab then value=1 \n",
    "        \n",
    "        vocab_dict = {}\n",
    "\n",
    "        for word in doc:\n",
    "            if word in vocab.keys():\n",
    "                vocab_dict[word] = 1\n",
    "            else:\n",
    "                vocab_dict['<unk>'] = 1                \n",
    "       \n",
    "        return vocab_dict\n",
    "                \n",
    "                \n",
    "         \n",
    "        #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoWFeaturizer(object):\n",
    "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
    "        \"\"\"\n",
    "        Given a document represented as a list of tokens and the vocabulary\n",
    "        as a set of tokens, computed the count bag-of-words feature representation.\n",
    "        This function returns a dictionary which maps from the name of the\n",
    "        feature to the value of that feature.\n",
    "        \"\"\"\n",
    "    # for words present in doc, we will find the word in vocab and its count in doc\n",
    "        \n",
    "        vocab_dict={}\n",
    "\n",
    "        for word in doc:\n",
    "            if word in vocab.keys():\n",
    "                vocab_dict[word] = doc.count(word)\n",
    "            else:\n",
    "                if '<unk>' in vocab_dict.keys():\n",
    "                    vocab_dict['<unk>']+= doc.count(word)\n",
    "                else:\n",
    "                    vocab_dict['<unk>'] = doc.count(word)\n",
    "        \n",
    "\n",
    "        return vocab_dict     \n",
    "#         #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(D, vocab):\n",
    "    \"\"\"\n",
    "    Given a list of documents D and the vocabulary as a set of tokens,\n",
    "    where each document is represented as a list of tokens, returned the IDF scores\n",
    "    for every token in the vocab. The IDFs represented a dictionary that\n",
    "    maps from the token to the IDF value. If a token is not present in the\n",
    "    vocab, it was mapped to \"<unk>\".\n",
    "    \"\"\"\n",
    "    idf={}\n",
    "    num=len(D)\n",
    "\n",
    "    for i in range(0,len(D)):\n",
    "        unk_presence = False\n",
    "        for word in set(D[i]):\n",
    "            if word in vocab.keys():\n",
    "                if word in idf.keys():\n",
    "                    idf[word]+=1\n",
    "                else:\n",
    "                    idf[word]=1\n",
    "            else:\n",
    "                unk_presence = True\n",
    "                \n",
    "        if unk_presence:\n",
    "            if '<unk>' in idf.keys():\n",
    "                idf['<unk>']+= 1\n",
    "            else:\n",
    "                idf['<unk>'] = 1\n",
    "    \n",
    "    for word in vocab.keys():\n",
    "        if word not in idf.keys():\n",
    "            idf[word] = 0\n",
    "    \n",
    "    for word in idf.keys():\n",
    "        if idf[word]==0:\n",
    "            idf.pop(word)\n",
    "        else:\n",
    "            idf[word]=np.log(num/idf[word])\n",
    "\n",
    "    return idf\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "class TFIDFFeaturizer(object):\n",
    "    def __init__(self,idf):\n",
    "        \"\"\"The idf scores computed via `compute_idf`.\"\"\"\n",
    "        self.idf = idf\n",
    "    \n",
    "    \n",
    "    \n",
    "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
    "        \"\"\"\n",
    "        Given a document represented as a list of tokens and\n",
    "        the vocabulary as a set of tokens, computed\n",
    "        the TF-IDF feature representation. This function\n",
    "         returns a dictionary which maps from the name of the\n",
    "        feature to the value of that feature.\n",
    "        \"\"\"\n",
    "        tfidf={}\n",
    "#       using the count from CBOW\n",
    "        for word in doc:\n",
    "            if word in vocab.keys():\n",
    "                tfidf[word] = doc.count(word)\n",
    "            else:\n",
    "                if '<unk>' in tfidf.keys():\n",
    "                    tfidf['<unk>']+= doc.count(word)\n",
    "                else:\n",
    "                    tfidf['<unk>'] = doc.count(word)\n",
    "                \n",
    "                \n",
    "            \n",
    "        for word in tfidf.keys():\n",
    "            tfidf[word]=tfidf[word]*self.idf[word]   \n",
    "\n",
    "        return tfidf\n",
    "        #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    D = []\n",
    "    y = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            instance = json.loads(line)\n",
    "            D.append(instance['document'])\n",
    "            y.append(instance['label'])\n",
    "    return D, y\n",
    "\n",
    "def convert_to_features(D, featurizer, vocab):\n",
    "    X = []\n",
    "    for doc in D:\n",
    "        X.append(featurizer.convert_document_to_feature_dictionary(doc, vocab))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(X, y, k, vocab):\n",
    "    \"\"\"\n",
    "    Computes the statistics for the Naive Bayes classifier.\n",
    "    X is a list of feature representations, where each representation\n",
    "    is a dictionary that maps from the feature name to the value.\n",
    "    y is a list of integers that represent the labels.\n",
    "    k is a float which is the smoothing parameters.\n",
    "    vocab is the set of vocabulary tokens.\n",
    "    \n",
    "    Returns two values:\n",
    "        p_y: A dictionary from the label to the corresponding p(y) score\n",
    "        p_v_y: A nested dictionary where the outer dictionary's key is\n",
    "            the label and the innner dictionary maps from a feature\n",
    "            to the probability p(v|y). For example, `p_v_y[1][\"hello\"]`\n",
    "            should be p(v=\"hello\"|y=1).\n",
    "    \"\"\"\n",
    "    \n",
    "    #for class=1\n",
    "    p_y1 = sum(y_train)/len(y_train)\n",
    "    \n",
    "    #for class=0\n",
    "    p_y0= 1 - p_y1\n",
    "    \n",
    "    \n",
    "    p_y={\"0\":p_y0, \"1\":p_y1}\n",
    "    #separating for class 0 and class 1 \n",
    "    class0=[]\n",
    "    class1=[]\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if y[i]==0:\n",
    "            class0.append(X[i])\n",
    "        else:\n",
    "            class1.append(X[i])\n",
    "            \n",
    "   # calculating p_v_y for class 0 \n",
    "    prob_num0={}\n",
    "    for i in range(len(class0)):\n",
    "        for word in class0[i].keys():\n",
    "            if word not in prob_num0.keys():\n",
    "                prob_num0[word] = class0[i][word]\n",
    "            else:\n",
    "                prob_num0[word]+= class0[i][word]\n",
    "                \n",
    "    for word in vocab.keys():\n",
    "        if word not in prob_num0.keys():\n",
    "            prob_num0[word] = 0\n",
    "            \n",
    "                  \n",
    "    den0=sum(list(prob_num0.values())) + k*len(prob_num0.keys())\n",
    "#     den0=sum(list(prob_num0.values()))\n",
    "    for word in prob_num0.keys():\n",
    "        prob_num0[word]= (k + prob_num0[word])/den0\n",
    "        \n",
    "              \n",
    "    # calculating p_v_y for class 1\n",
    "    prob_num1={}\n",
    "    for i in range(len(class1)):\n",
    "        for word in class1[i].keys():\n",
    "            if word not in prob_num1.keys():\n",
    "                prob_num1[word] = class1[i][word]\n",
    "            else:\n",
    "                prob_num1[word]+= class1[i][word]\n",
    "                \n",
    "    for word in vocab.keys():\n",
    "        if word not in prob_num1.keys():\n",
    "            prob_num1[word] = 0\n",
    "            \n",
    "       \n",
    "    den1=sum(list(prob_num1.values())) + k*len(prob_num1.keys())\n",
    "    \n",
    "    for word in prob_num1.keys():\n",
    "        prob_num1[word]= (k + prob_num1[word])/den1\n",
    "    \n",
    "            \n",
    "    p_v_y={\"0\":prob_num0, \"1\":prob_num1} \n",
    "    \n",
    "    return p_y , p_v_y\n",
    "\n",
    "    #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(D, p_y, p_v_y):\n",
    "    \"\"\"\n",
    "    Runs the prediction rule for Naive Bayes. D is a list of documents,\n",
    "    where each document is a list of tokens.\n",
    "    p_y and p_v_y are output from `train_naive_bayes`.\n",
    "    \n",
    "    Any token which is not in p_v_y is mapped to\n",
    "    \"<unk>\". Further, the input dictionaries are probabilities. I converted them to log-probabilities while I computed\n",
    "    the Naive Bayes prediction rule to prevent underflow errors.\n",
    "    \n",
    "    Returns two values:\n",
    "        predictions: A list of integer labels, one for each document,\n",
    "            that is the predicted label for each instance.\n",
    "        confidences: A list of floats, one for each document, that is\n",
    "            p(y|d) for the corresponding label that is returned.\n",
    "    \"\"\"\n",
    "  \n",
    "    log_p_y= {}\n",
    "    log_p_y[\"0\"]=np.log(p_y[\"0\"])\n",
    "    log_p_y[\"1\"]=np.log(p_y[\"1\"])\n",
    "    \n",
    "    prob_num0={}\n",
    "    prob_num1={}\n",
    "    \n",
    "    \n",
    "    for word in p_v_y[\"0\"].keys():\n",
    "        prob_num0[word]= np.log(p_v_y[\"0\"][word])\n",
    "        \n",
    "    for word in p_v_y[\"1\"].keys():\n",
    "        prob_num1[word]= np.log(p_v_y[\"1\"][word])\n",
    "        \n",
    "    log_p_v_y= {\"0\":prob_num0, \"1\":prob_num1}\n",
    "    \n",
    "    predictions=[]\n",
    "    for doc in D:\n",
    "        sum0=0\n",
    "        sum1=0\n",
    "        for word in doc:\n",
    "            if (word not in log_p_v_y[\"0\"].keys()) and (word not in log_p_v_y[\"1\"].keys()) :\n",
    "                sum0+= log_p_v_y[\"0\"][\"<unk>\"]\n",
    "                sum1+= log_p_v_y[\"1\"][\"<unk>\"]\n",
    "            elif word not in log_p_v_y[\"0\"].keys():\n",
    "                sum0+=  log_p_v_y[\"0\"][\"<unk>\"]      \n",
    "                sum1+= log_p_v_y[\"1\"][word]\n",
    "            elif word not in log_p_v_y[\"1\"].keys():\n",
    "                sum0+= log_p_v_y[\"0\"][word]\n",
    "                sum1+=  log_p_v_y[\"1\"][\"<unk>\"]  \n",
    "            else:\n",
    "                sum0+= log_p_v_y[\"0\"][word]\n",
    "                sum1+= log_p_v_y[\"1\"][word]\n",
    "            \n",
    "        sum0= sum0+ log_p_y[\"0\"]\n",
    "        sum1= sum1+ log_p_y[\"1\"]\n",
    "        \n",
    "        if sum0 > sum1:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(1)\n",
    "            \n",
    "    return predictions\n",
    "    \n",
    "    #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train, y_train = load_dataset('data/imdb/train.jsonl')\n",
    "D_valid, y_valid = load_dataset('data/imdb/valid.jsonl')\n",
    "D_test, y_test = load_dataset('data/imdb/test.jsonl')\n",
    "vocab = get_vocabulary(D_train)\n",
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computed the features, for example, using the BBowFeaturizer.\n",
    "# Converted the training instances to their feature-based representations.\n",
    "import time\n",
    "j = time.time()\n",
    "k=0.001\n",
    "vocab = get_vocabulary(D_train)\n",
    "featurizer = BBoWFeaturizer()\n",
    "X_train = convert_to_features(D_train, featurizer, vocab) \n",
    "\n",
    "k=0.01\n",
    "print(\"k=0.01\")\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "\n",
    "k=0.1\n",
    "print(\"k=0.1\")\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "k=1\n",
    "print(\"k=1\")\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "k=10\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "print((time.time() - j)/60)\n",
    "print(time.time() - j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the CBowFeaturizer.\n",
    "\n",
    "import time\n",
    "j = time.time()\n",
    "k=0.001\n",
    "vocab = get_vocabulary(D_train)\n",
    "featurizer = CBoWFeaturizer()\n",
    "\n",
    "X_train = convert_to_features(D_train, featurizer, vocab) \n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "\n",
    "k=0.01\n",
    "print(\"k=0.01\")\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "k=0.1\n",
    "print(\"k=0.1\")\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "# print(y_pred)\n",
    "\n",
    "k=1\n",
    "print(\"k=1\")\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "k=10\n",
    "\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "print((time.time() - j)/60)\n",
    "print(time.time() - j)\n",
    "\n",
    "print((time.time() - j)/60)\n",
    "print(time.time() - j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  using the TFIDFFeaturizer.\n",
    "import time\n",
    "j = time.time()\n",
    "k=0.001\n",
    "print(\"k=0.001\")\n",
    "\n",
    "idf=compute_idf(D_train, vocab)\n",
    "featurizer = TFIDFFeaturizer(idf)\n",
    "X_train = convert_to_features(D_train, featurizer, vocab) \n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "k=0.01\n",
    "print(\"k=0.01\")\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "k=0.1\n",
    "print(\"k=0.1\")\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "# print(y_pred)\n",
    "\n",
    "k=1\n",
    "\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "k=10\n",
    "p_y , p_v_y= train_naive_bayes(X_train, y_train, k, vocab)\n",
    "\n",
    "y_pred_valid= predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "print(\"Validation accuracy: \", accuracy_score(y_pred_valid, y_valid))\n",
    "\n",
    "y_pred_test = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "print(\"Test accuracy: \", accuracy_score(y_pred_test, y_test))\n",
    "print((time.time() - j)/60)\n",
    "print(time.time() - j)\n",
    "\n",
    "print((time.time() - j)/60)\n",
    "print(time.time() - j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
