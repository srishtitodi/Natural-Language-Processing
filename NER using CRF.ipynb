{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "def read_data(filename):\n",
    "    rows = []\n",
    "    with open(f'ner/wnut16/{filename}',encoding=\"utf8\") as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) < 2:\n",
    "                continue\n",
    "            rows.append(line.rstrip('\\n').split())\n",
    "    data = pd.DataFrame(rows, columns=['term', 'entitytags'])\n",
    "    data[\"pos\"]=nltk.pos_tag(data[\"term\"])\n",
    "    for i in range(len(data)):\n",
    "        data[\"pos\"][i]=data[\"pos\"][i][1]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('train')\n",
    "test = read_data('test')\n",
    "dev = read_data('dev')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process to get the train, test, dev dataset for crf\n",
    "\n",
    "def process_data(data):\n",
    "    dataset = []\n",
    "    sent = []\n",
    "    for i, (term, entitytags,pos) in data.iterrows():\n",
    "        if term == '.':\n",
    "            sent.append((term, entitytags,pos))\n",
    "            dataset.append(sent)\n",
    "            sent = []\n",
    "        else:\n",
    "            sent.append((term, entitytags,pos))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = process_data(train)\n",
    "test_sents = process_data(test)\n",
    "dev_sents = process_data(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following function will design the feature for crf model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "\n",
    "    features = {\n",
    "        \"word.isupper()\": word.isupper(),\n",
    "        'word.lower()': word.lower(),\n",
    "        \"word.postags\": sent[i][2],\n",
    "        \"word.istitle()\":word.istitle(),\n",
    "        \"word.isdigit()\": word.isdigit(),\n",
    "        \"word[-3:]\": word[-3:],\n",
    "        \"word[:2]\" : word[:2],\n",
    "        \"len_word\": len(word)\n",
    "    }\n",
    "    \n",
    "    if i>0 :\n",
    "        word1=sent[i-1][0]\n",
    "        features.update({\"word1.isupper()\": word1.isupper(),\n",
    "                         'word1.lower()': word1.lower(),\n",
    "\n",
    "        \"word1.postags\": sent[i-1][2],\n",
    "        \"word1.istitle()\":word1.istitle(),\n",
    "        \"word1.isdigit()\": word1.isdigit(),\n",
    "         \"word1[-3:]\": word1[-3:],\n",
    "         \"word1[:2]\" : word1[:2],\n",
    "         \"len_word1\": len(word1)               }     \n",
    "                       )\n",
    "        \n",
    "    else:\n",
    "        features[\"BOS\"]=True\n",
    "        \n",
    "        \n",
    "    if i < (len(sent) -1) :\n",
    "        word2=sent[i+1][0]\n",
    "        features.update({\"word2.isupper()\": word2.isupper(),\n",
    "                         'word2.lower()': word2.lower(),\n",
    "\n",
    "        \"word2.postags\": sent[i-1][2],\n",
    "        \"word2.istitle()\":word2.istitle(),\n",
    "        \"word2.isdigit()\": word2.isdigit(),\n",
    "        \"word2[-3:]\": word2[-3:],\n",
    "        \"word2[:2]\" : word2[:2],\n",
    "         \"len_word2\": len(word2)                }              )\n",
    "        \n",
    "    else:\n",
    "        features[\"EOS\"]=True\n",
    "        \n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label, tags in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label, tags in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "X_dev = [sent2features(s) for s in dev_sents]\n",
    "y_dev = [sent2labels(s) for s in dev_sents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block of code, we use try and except because the version of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code will help visualize the learned features for crf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [sent2tokens(s) for s in test_sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [sent2labels(s) for s in test_sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for (word, true_id, pred_id) in zip(words, labels, y_pred):\n",
    "    for (w, t, p) in zip(word, true_id, pred_id):\n",
    "        line = ' '.join([w, t, p])\n",
    "        predictions.append(line)\n",
    "    predictions.append('')\n",
    "with open('crf_pred', 'w',encoding=\"utf8\") as f:\n",
    "    f.write('\\n'.join(predictions))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "eval_script = '../released/src/conlleval.pl'\n",
    "predf = 'crf_pred'\n",
    "scoref = 'crf_score'\n",
    "# os.system('%s < %s > %s' % (eval_script, predf, scoref))\n",
    "os.system('perl %s < %s > %s' % (eval_script, predf, scoref))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lines = [l.rstrip() for l in open(scoref, 'r', encoding='utf8')]\n",
    "\n",
    "for i, line in enumerate(eval_lines):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check what classifier learned:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the state features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gmb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "import time\n",
    "start= time.time()\n",
    "def read_data(filename):\n",
    "    rows = []\n",
    "    with open(f'ner/GMB/{filename}',encoding=\"utf8\") as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) < 2:\n",
    "                continue\n",
    "            rows.append(line.rstrip('\\n').split())\n",
    "    data = pd.DataFrame(rows, columns=['term', 'entitytags'])\n",
    "    # add the pos tags to the dataframe\n",
    "    # some lines of codes\n",
    "    data[\"pos\"]=nltk.pos_tag(data[\"term\"])\n",
    "    for i in range(len(data)):\n",
    "        data[\"pos\"][i]=data[\"pos\"][i][1]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('train')\n",
    "test = read_data('test')\n",
    "dev = read_data('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process to get the train, test, dev dataset for crf\n",
    "def process_data(data):\n",
    "    dataset = []\n",
    "    sent = []\n",
    "    for i, (term, entitytags,pos) in data.iterrows():\n",
    "        if term == '.':\n",
    "            sent.append((term, entitytags,pos))\n",
    "            dataset.append(sent)\n",
    "            sent = []\n",
    "        else:\n",
    "            sent.append((term, entitytags,pos))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = process_data(train)\n",
    "test_sents = process_data(test)\n",
    "dev_sents = process_data(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "\n",
    "    features = {\n",
    "        \"word.isupper()\": word.isupper(),\n",
    "        'word.lower()': word.lower(),\n",
    "        # add more features here\n",
    "        \"word.postags\": sent[i][2],\n",
    "        \"word.istitle()\":word.istitle(),\n",
    "        \"word.isdigit()\": word.isdigit(),\n",
    "        \"word[-3:]\": word[-3:],\n",
    "        \"word[:2]\" : word[:2],\n",
    "        \"len_word\": len(word)\n",
    "    }\n",
    "    \n",
    "    if i>0 :\n",
    "        word1=sent[i-1][0]\n",
    "        features.update({\"word1.isupper()\": word1.isupper(),\n",
    "                         'word1.lower()': word1.lower(),\n",
    "\n",
    "        \"word1.postags\": sent[i-1][2],\n",
    "        \"word1.istitle()\":word1.istitle(),\n",
    "        \"word1.isdigit()\": word1.isdigit(),\n",
    "         \"word1[-3:]\": word1[-3:],\n",
    "         \"word1[:2]\" : word1[:2],\n",
    "         \"len_word1\": len(word1)               }     \n",
    "                       )\n",
    "        \n",
    "    else:\n",
    "        features[\"BOS\"]=True\n",
    "        \n",
    "        \n",
    "    if i < (len(sent) -1) :\n",
    "        word2=sent[i+1][0]\n",
    "        features.update({\"word2.isupper()\": word2.isupper(),\n",
    "                         'word2.lower()': word2.lower(),\n",
    "\n",
    "        \"word2.postags\": sent[i-1][2],\n",
    "        \"word2.istitle()\":word2.istitle(),\n",
    "        \"word2.isdigit()\": word2.isdigit(),\n",
    "        \"word2[-3:]\": word2[-3:],\n",
    "        \"word2[:2]\" : word2[:2],\n",
    "         \"len_word2\": len(word2)                }              )\n",
    "        \n",
    "    else:\n",
    "        features[\"EOS\"]=True\n",
    "        \n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label, tags in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label, tags in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "X_dev = [sent2features(s) for s in dev_sents]\n",
    "y_dev = [sent2labels(s) for s in dev_sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "end= time.time()-start\n",
    "print(end//60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [sent2tokens(s) for s in test_sents]\n",
    "labels = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c6df9a31e22d349e1e2c332942a3685d26451eadac46d27ac0f206f2387928f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
