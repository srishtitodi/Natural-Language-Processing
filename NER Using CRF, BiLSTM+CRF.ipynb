{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import loader as loader\n",
    "from train import *\n",
    "from loader import *\n",
    "from model import NERModel\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.ArgumentParser()\n",
    "\n",
    "args.add_argument(\"--dataset\", default=\"wnut16\", choices=[\"GMB\", \"wnut16\"])\n",
    "args.add_argument(\"--use_gpu\", action=\"store_true\", default=False)\n",
    "args.add_argument(\"--word_dim\", type=int, default=100)\n",
    "args.add_argument(\"--pre_emb\", default=\"src/glove.6B.100d.txt\")\n",
    "args.add_argument(\"--lstm_dim\", type=int, default=300)\n",
    "args.add_argument(\"--epoch\", type=int, default=10)\n",
    "args.add_argument(\"--use_crf\", action=\"store_true\", default=False)\n",
    "args.add_argument(\"--batch_size\", type=int, default=32)\n",
    "args.add_argument(\"--num_layers\", type=int, default=2)\n",
    "args.add_argument(\"--num_workers\", type=int, default=4)\n",
    "args.add_argument(\"--dropout\", type=int, default=0.1)\n",
    "args = args.parse_args([])\n",
    "\n",
    "args.train = \"../released/ner/\" + args.dataset + \"/train\"\n",
    "args.test = \"../released/ner/\" + args.dataset + \"/test\"\n",
    "args.dev = \"../released/ner/\" + args.dataset + \"/dev\"\n",
    "use_gpu = args.use_gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "\n",
    "train_sentences = loader.load_sentences(args.train)\n",
    "dev_sentences = loader.load_sentences(args.dev)\n",
    "test_sentences = loader.load_sentences(args.test)\n",
    "\n",
    "word2id, id2word = word_mapping(\n",
    "    train_sentences, test_sentences, dev_sentences)\n",
    "tag2id, id2tag = tag_mapping(\n",
    "    train_sentences, test_sentences, dev_sentences)\n",
    "\n",
    "train_set = NERDataset(train_sentences, word2id, tag2id)\n",
    "test_set = NERDataset(test_sentences, word2id, tag2id)\n",
    "dev_set = NERDataset(dev_sentences, word2id, tag2id)\n",
    "\n",
    "train_data = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers,\n",
    "                        collate_fn=train_set.collate_fn)\n",
    "\n",
    "test_data = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers,\n",
    "                        collate_fn=test_set.collate_fn)\n",
    "\n",
    "dev_data = DataLoader(dev_set, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers,\n",
    "                        collate_fn=dev_set.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(id2tag.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_embeds = {}\n",
    "word_dim = args.word_dim\n",
    "if args.pre_emb:\n",
    "    for i, line in enumerate(open(args.pre_emb, \"r\", encoding=\"utf-8\")):\n",
    "        s = line.strip().split()\n",
    "        word_dim = len(s) - 1\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "    print(\"Loaded %i pretrained embeddings.\" % len(all_word_embeds))\n",
    "\n",
    "word_embeds = np.random.uniform(-np.sqrt(0.06),\n",
    "                                np.sqrt(0.06), (len(word2id), word_dim))\n",
    "\n",
    "for w in word2id:\n",
    "    if w in all_word_embeds:\n",
    "        word_embeds[word2id[w]] = all_word_embeds[w]\n",
    "    elif w.lower() in all_word_embeds:\n",
    "        word_embeds[word2id[w]] = all_word_embeds[w.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NERModel(\n",
    "        vocab_size=len(word2id),\n",
    "        tag_to_ix=tag2id,\n",
    "        embedding_dim=word_dim,\n",
    "        hidden_dim=args.lstm_dim,\n",
    "        num_laters=args.num_layers,\n",
    "        dropout= args.dropout,\n",
    "        pre_word_embeds=word_embeds,\n",
    "        use_gpu=args.use_gpu,\n",
    "        use_crf=args.use_crf,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "if args.use_gpu:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "train(model, args.epoch, train_data, dev_data,\n",
    "        test_data, use_gpu=args.use_gpu, id_to_tag=id2tag)\n",
    "\n",
    "print((time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c6df9a31e22d349e1e2c332942a3685d26451eadac46d27ac0f206f2387928f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
