{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MiUGBKK2ynjh"
   },
   "outputs": [],
   "source": [
    "#Impporting all the libraries\n",
    "import json\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Xqyfj9mAynji"
   },
   "outputs": [],
   "source": [
    "#CReating a function for calculating F1 score from scratch\n",
    "def confusion_matrix(actual,predicted): \n",
    "    x=np.zeros((2,2))\n",
    "    for i in range(len(actual)):\n",
    "        if int(actual[i])==-1 and int(predicted[i])==-1:\n",
    "            x[0][0]+=1     #true negatives\n",
    "        elif int(actual[i])==-1 and int(predicted[i])==1:\n",
    "            x[0][1]+=1   #false positive\n",
    "        elif int(actual[i])==1 and int(predicted[i])==-1:\n",
    "            x[1][0]+=1    #false negaitve   \n",
    "        elif int(actual[i])==1 and int(predicted[i])==1:\n",
    "            x[1][1]+=1       #true positive\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "def calculate_f1(y_gold, y_model):\n",
    "    conf_matrix= confusion_matrix(y_gold, y_model)\n",
    "    precision= conf_matrix[1][1]/(conf_matrix[0][1]+conf_matrix[1][1])\n",
    "    recall=conf_matrix[1][1]/(conf_matrix[1][0]+conf_matrix[1][1])\n",
    "    f1= (2*precision*recall)/(precision+recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6LjgnmXwynjj"
   },
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "\n",
    "    def train(self, X, y):\n",
    "        iterations = 10\n",
    "        for iteration in range(iterations):\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                self.process_example(x_i, y_i)\n",
    "            \n",
    "        self.finalize()\n",
    "\n",
    "    def process_example(self, x, y):\n",
    "        \"\"\"\n",
    "        Makes a prediction using the current parameter values for\n",
    "        the features x and potentially updates the parameters based\n",
    "        on the gradient. \"x\" is a dictionary which maps from the feature\n",
    "        name to the feature value and y is either 1 or -1.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"Calculates the final parameter values for the averaged models.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts labels for all of the input examples.\n",
    "        \"\"\"\n",
    "        y = []\n",
    "        for x in X:\n",
    "            y.append(self.predict_single(x))\n",
    "        return y\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predicts a label, 1 or -1, for the input example. \"x\" is a dictionary\n",
    "        which maps from the feature name to the feature value.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ALNBBAbrynjj"
   },
   "outputs": [],
   "source": [
    "class Perceptron(Classifier):\n",
    "    def __init__(self, features):\n",
    "        \"\"\"\n",
    "        Initializes the parameters for the Perceptron model. \"features\"\n",
    "        is a list of all of the features of the model where each is\n",
    "        represented by a string.\n",
    "        \"\"\"\n",
    "        # Do not change the names of these 3 data members \n",
    "        self.eta = 1\n",
    "        self.w = {feature: 0.0 for feature in features}\n",
    "        self.theta = 0\n",
    "\n",
    "    def process_example(self, x, y):\n",
    "        y_pred = self.predict_single(x)\n",
    "        if y != y_pred:\n",
    "            for feature, value in x.items():\n",
    "                self.w[feature] += self.eta * y * value\n",
    "            self.theta += self.eta * y\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Je_X7gjJynjl"
   },
   "outputs": [],
   "source": [
    "class AveragedPerceptron(Classifier):\n",
    "    def __init__(self, features):\n",
    "        self.eta = 1\n",
    "        self.w = {feature: 0.0 for feature in features}                        #initialising weight\n",
    "        self.averageweight={feature: 0.0 for feature in features}               #initialising average weight           \n",
    "        self.theta = 0                                                          #initialising bias\n",
    "        self.averagetheta=0                                                     #initialising average bias\n",
    "        self.counter= 1                                                        #setting the counter to 1 initially\n",
    "        # You will need to add data members here\n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        y_pred = self.predict_single(x)\n",
    "        if y != y_pred:\n",
    "            for feature, value in x.items():\n",
    "                self.w[feature] += self.eta * y * value                                 #updating the weights\n",
    "                self.averageweight[feature] += self.eta * y * value*self.counter       #updating the average weights\n",
    "            self.theta += self.eta * y                                                 #updating the bias\n",
    "            self.averagetheta += self.eta * y*self.counter                             #updating the average bias\n",
    "            self.counter+=1                                            #incremeting the counter irrespective of misclassification\n",
    "\n",
    "        \n",
    "\n",
    "    def predict_single(self, x):\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    def finalize(self):\n",
    "        for feature, weights in self.w.items():\n",
    "            self.w[feature] = self.w[feature] - self.averageweight[feature]/self.counter     #updating the final averaged weight \n",
    "        self.theta = self.theta - self.averagetheta/self.counter                             #updating the final averaged bias\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5B8f3Il9ynjl"
   },
   "outputs": [],
   "source": [
    "def load_ner_data(path):\n",
    "    \n",
    "#     Loads the NER data from a path (e.g. \"ner/conll/train\").\n",
    "    # List of tuples for each sentence\n",
    "    data = []\n",
    "    for filename in os.listdir(path):\n",
    "        with open(path + '/' + filename, 'r') as file:\n",
    "            sentence = []\n",
    "            for line in file:\n",
    "                if line == '\\n':\n",
    "                    data.append(sentence)\n",
    "                    sentence = []\n",
    "                else:\n",
    "                    sentence.append(tuple(line.split()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-AIc8SJeynjl"
   },
   "outputs": [],
   "source": [
    "def extract_ner_features_train(train):\n",
    "    \"\"\"\n",
    "    Extracts feature dictionaries and labels from the data in \"train\"\n",
    "    Additionally creates a list of all of the features which were created.\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    X = []\n",
    "    features = set()\n",
    "    for sentence in train:\n",
    "        padded = sentence[:]\n",
    "        padded.insert(0, ('SSS', None))\n",
    "        padded.append(('EEE', None))\n",
    "        \n",
    "        for i in range(1, len(padded) - 1):\n",
    "            feats = []                                  #creating an empty list here which will be uodated with each condition\n",
    "            y.append(1 if padded[i][1] == 'I' else -1)\n",
    "            feat1 = 'w-1=' + str(padded[i - 1][0])\n",
    "            feat2 = 'w+1=' + str(padded[i + 1][0])\n",
    "            feats.append(feat1)\n",
    "            feats.append(feat2)\n",
    "            if i-2>=0:                                         #setting the conditions so that the list doesn't go out of bound\n",
    "                feat3 = 'w-2=' + str(padded[i - 2][0])               \n",
    "                feats.append(feat3)             \n",
    "            if i+2<= len(padded)-1:                   #since the range in our condition is len-1 so i+2 should be less than that\n",
    "                feat4 = 'w+2=' + str(padded[i + 2][0])\n",
    "                feats.append(feat4)\n",
    "            if i-3>=0:\n",
    "                feat5 = 'w-3=' + str(padded[i - 3][0])\n",
    "                feats.append(feat5)\n",
    "            if i+3<= len(padded)-1:\n",
    "                feat6 = 'w+3=' + str(padded[i + 3][0])\n",
    "                feats.append(feat6)\n",
    "            if i-1>=0 and i-2>=0:\n",
    "                feat7 = 'w-1=' + str(padded[i - 1][0]) + \"&w-2=\" + str(padded[i - 2][0])\n",
    "                feats.append(feat7)\n",
    "            if i+2<= len(padded)-1 and i+1<= len(padded)-1:\n",
    "                feat8 = 'w+1=' + str(padded[i + 1][0]) + \"&w+2=\" + str(padded[i + 2][0])\n",
    "                feats.append(feat8)\n",
    "            if i+1<= len(padded)-1 and i-1>=0:\n",
    "                feat9 = 'w-1=' + str(padded[i - 1][0]) + \"&w+1=\" + str(padded[i + 1][0])\n",
    "                feats.append(feat9)\n",
    "            features.update(feats)\n",
    "            feats = {feature: 1 for feature in feats}\n",
    "            X.append(feats)\n",
    "    return features, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TiSqgbUqynjm"
   },
   "outputs": [],
   "source": [
    "def extract_features_dev_or_test(data, features):\n",
    "    \"\"\"\n",
    "    Extracts feature dictionaries and labels from \"data\". The only\n",
    "    features which should be computed are those in \"features\".\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    X = []\n",
    "    for sentence in data:\n",
    "        padded = sentence[:]\n",
    "        padded.insert(0, ('SSS', None))\n",
    "        padded.append(('EEE', None))\n",
    "        \n",
    "        for i in range(1, len(padded) - 1):\n",
    "            feats=[]\n",
    "            y.append(1 if padded[i][1] == 'I' else -1)\n",
    "            feat1 = 'w-1=' + str(padded[i - 1][0])\n",
    "            feat2 = 'w+1=' + str(padded[i + 1][0])\n",
    "            feats.append(feat1)\n",
    "            feats.append(feat2)\n",
    "            if i-2>=0:\n",
    "                feat3 = 'w-2=' + str(padded[i - 2][0])\n",
    "                feats.append(feat3)\n",
    "            if i+2<= len(padded) - 1:\n",
    "                feat4 = 'w+2=' + str(padded[i + 2][0])\n",
    "                feats.append(feat4)\n",
    "            if i-3>=0:\n",
    "                feat5 = 'w-3=' + str(padded[i - 3][0])\n",
    "                feats.append(feat5)\n",
    "            if i+3<= len(padded)-1:\n",
    "                feat6 = 'w+3=' + str(padded[i + 3][0])\n",
    "                feats.append(feat6)\n",
    "            if i-1>=0 and i-2>=0:\n",
    "                feat7 = 'w-1=' + str(padded[i - 1][0]) + \"&w-2=\" + str(padded[i - 2][0])\n",
    "                feats.append(feat7)\n",
    "            if i+2<= len(padded)-1 and i+1<= len(padded)-1:\n",
    "                feat8 = 'w+1=' + str(padded[i + 1][0]) + \"&w+2=\" + str(padded[i + 2][0])\n",
    "                feats.append(feat8)\n",
    "            if i+1<= len(padded)-1 and i-1>=0:\n",
    "                feat9 = 'w-1=' + str(padded[i - 1][0]) + \"&w+1=\" + str(padded[i + 1][0])\n",
    "                feats.append(feat9)\n",
    "            feats = {feature: 1 for feature in feats if feature in features}\n",
    "            X.append(feats)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Y5VN_Sv3ynjm"
   },
   "outputs": [],
   "source": [
    "def run_ner_experiment(data_path):\n",
    "    \"\"\"\n",
    "    Runs the NER experiment using the path to the ner data\n",
    "    (e.g. \"ner\" from resources). \n",
    "    \"\"\"\n",
    "    train = load_ner_data(data_path + '/conll/train')\n",
    "    conll_test = load_ner_data(data_path + '/conll/test')\n",
    "    enron_test = load_ner_data(data_path + '/enron/test')\n",
    "\n",
    "    features, X_train, y_train = extract_ner_features_train(train)\n",
    "    X_conll_test, y_conll_test = extract_features_dev_or_test(conll_test, features)\n",
    "    X_enron_test, y_enron_test = extract_features_dev_or_test(enron_test, features)\n",
    "                 \n",
    "    \n",
    "    classifier = Perceptron(features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred_conll = classifier.predict(X_conll_test)\n",
    "    conll_f1_perceptron = calculate_f1(y_conll_test, y_pred_conll)\n",
    "    y_pred_enron = classifier.predict(X_enron_test)\n",
    "    enron_f1_perceptron = calculate_f1(y_enron_test, y_pred_enron)\n",
    "\n",
    "    classifier = AveragedPerceptron(features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred_conll = classifier.predict(X_conll_test)\n",
    "    conll_f1_avgperceptron = calculate_f1(y_conll_test, y_pred_conll)\n",
    "    y_pred_enron = classifier.predict(X_enron_test)\n",
    "    enron_f1_avgperceptron = calculate_f1(y_enron_test, y_pred_enron)\n",
    "\n",
    "    classifier = LinearSVC(loss='hinge')\n",
    "    vectorizer = DictVectorizer()\n",
    "    X_train_dict = vectorizer.fit_transform(X_train)\n",
    "    X_conll_test_dict = vectorizer.transform(X_conll_test)     #Converting the conll and enron data to the required form \n",
    "    X_enron_test_dict = vectorizer.transform(X_enron_test)    \n",
    "    classifier.fit(X_train_dict, y_train)\n",
    "    y_pred_conll = classifier.predict(X_conll_test_dict)       #predicting for conll \n",
    "    conll_f1_svm = calculate_f1(y_conll_test, y_pred_conll)\n",
    "    y_pred_enron = classifier.predict(X_enron_test_dict)\n",
    "    enron_f1_svm = calculate_f1(y_enron_test, y_pred_enron)\n",
    "    # to print the f1 score in a table, I will create a dataframe here\n",
    "    data={\"Algorithms\":[\"Perceptron\",\"Average Perceptron\",\"SVM\"], \"ConLL_F1_score\":[conll_f1_perceptron,conll_f1_avgperceptron,conll_f1_svm], \"Enron_F1_score\":[enron_f1_perceptron,enron_f1_avgperceptron,enron_f1_svm]}\n",
    "    df=pd.DataFrame(data)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\srish\\\\Downloads\\\\nlp assignment1\\\\released'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()                #getting the path of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "D8DwDwL4ynjm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Algorithms  ConLL_F1_score  Enron_F1_score\n",
      "0          Perceptron        0.747623        0.200903\n",
      "1  Average Perceptron        0.807854        0.218876\n",
      "2                 SVM        0.827824        0.240682\n"
     ]
    }
   ],
   "source": [
    "# Run the NER experiment. \"ner\" is the path to where the data is located.\n",
    "features = run_ner_experiment('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on the result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The three algorithms can be compared using the f1 scores obtained for the test data. We see that the f1 score for SVM algorithm is the highest for ConLL and Enron dataset. Thus it performs the best. It is better than perceptron based algortihms as it finds the best plane with the maximum margin and doesn't stop after correctly classifying data like perceptron. \n",
    "\n",
    "#### The f1 score decreased drastically for Enron data. One of the reasons could be the nature of the train and test data. The datasets come from entirely different backgrounds. Thus, there will be difference in the nature of the sentences on which we have trained our model(there will be new and unknown features for Enron data) . The other reason could be overfitting of the model on the train dataset which results in high score for similar test data and drastically low for data from other background. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
